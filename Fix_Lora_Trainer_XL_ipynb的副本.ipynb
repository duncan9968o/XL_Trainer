{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omCLgUOB2_ZF"
      },
      "source": [
        "# 🌟 XL Lora Trainer\n",
        "\n",
        "❗ 推薦使用 Colab Premium。理想情況下，您可以將運行時間變更為 A100 並使用最大批次大小。 但是，如果您載入diffusers，您仍然可以免費訓練，只是需要更長的時間。\n",
        "\n",
        "\n",
        "This colab is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://github.com/Linaqruf/kohya-trainer). Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ⭕ 免責聲明\n",
        "本文件的目的是研究機器學習推理領域的前沿技術。  \n",
        "請閱讀並遵守 [Google Colab 指南](https://research.google.com/colaboratory/faq.html) 和其 [服務條款](https://research.google.com/colaboratory/tos_v3.html)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|TrainerXL|Dataset Maker WDV2|Dataset Maker WDV3| |\n",
        "|:--|:-:|:-:|:-:||:-:|\n",
        "| 🏠 **Original Proyect** |[![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | |\n",
        "|**Modified By WhiteZ**| [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/gwhitez/Lora-Trainer-XL) |   [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb) |[![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Dataset_Maker%20By%20WhiteZ.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Waifu_Diffusion_V3_Dataser_Maker.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT1WRbhiHRCJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "import time\n",
        "\n",
        "#@title ## 🚩 Start Here\n",
        "#auto off by ZeroProtecPlus https://github.com/ZeroProtecPlus\n",
        "#@markdown ### 💤 Auto Off\n",
        "\n",
        "#@markdown 如果選中此選項，訓練完成後環境將自動斷開連接。\n",
        "auto_off   = True  # @param {type: \"boolean\"}\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido\\033[0m\")\n",
        "if not auto_off:\n",
        "    print(\"\\033[96mAuto Off apagado\\033[0m\")\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/qaneel/kohya-trainer\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ### ▶️ 設定\n",
        "#@markdown 您的專案名稱將與包含圖片的資料夾名稱相同。不能包含空格，如果名稱過長，可以使用 `底線`。\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown 資料夾結構無關緊要，純粹是為了方便。請確保始終選擇相同的結構。我喜歡按專案進行整理。\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "#@markdown 選擇要下載並用於訓練的模型。您也可以選擇自己的模型，粘貼其下載連結或從您的 Google Drive 中使用以 `/content/drive/MyDrive` 開頭的檔案。\n",
        "training_model = \"WAI-NSFW-illustrious\" # @param [\"Pony Diffusion V6 XL\",\"Animagine XL V3\",\"Illustrious\",\"NoobAI-XL0.75\",\"NoobAI-XL0.5\",\"Stable Diffusion XL 1.0 base\",\"Pony_Realism\",\"WAI-NSFW-illustrious\"]\n",
        "optional_custom_training_model_url = \"\" #@param {type:\"string\"}\n",
        "#@markdown 使用 diffusers 模型會消耗較少資源。上述選項在使用或不使用 diffusers 時都可以運行。\n",
        "load_diffusers = True #@param {type:\"boolean\"}\n",
        "#@markdown 如果您希望隨時間可視化訓練進度，請使用 wandb。\n",
        "wandb_key = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "if optional_custom_training_model_url:\n",
        "  model_url = optional_custom_training_model_url\n",
        "elif \"Pony\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/Pony_diffusion_v6_diffusers_fp16\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/PonyXL/resolve/main/PonyDiffusionV6XL.safetensors\"\n",
        "  model_file = \"/content/ponyDiffusionV6XL.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
        "  model_file = \"/content/animagineXLV3.safetensors\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
        "elif \"WAI-NSFW-illustrious\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://civitai.com/api/download/models/1183765?type=Model&format=SafeTensor&size=pruned&fp=fp16\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/1183765?type=Model&format=SafeTensor&size=pruned&fp=fp16\"\n",
        "elif \"NoobAI-XL0.75\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75/resolve/main/NoobAI-XL-v0.75.safetensors\"\n",
        "elif \"NoobAI-XL0.5\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5/resolve/main/NoobAI-XL-v0.5.safetensors\"\n",
        "else:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stablediffusionapi/pony-realism\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/LyliaEngine/ponyRealism_v21MainVAE/resolve/main/ponyRealism_v21MainVAE.safetensors\"\n",
        "  model_file = \"/content/ponyrealism.safetensors\"\n",
        "\n",
        "if load_diffusers:\n",
        "  vae_file= \"stabilityai/sdxl-vae\"\n",
        "else:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "  vae_file = \"/content/sdxl_vae.safetensors\"\n",
        "\n",
        "\n",
        "#@markdown ### ▶️ 處理\n",
        "#@markdown 預設角色的解析度為 1024。其他可選解析度包括 896（建議用於角色，或保持 1024）以及 768（建議用於風格，該解析度可使用更多的重複次數）。\n",
        "resolution = 1024 #@param {type:\"number\", min:768, max:1536, step:128}\n",
        "#@markdown 如果您的數據集較小，啟用 `Flip Aug` 會很有幫助，適用於等距角色。它會將所有圖片進行鏡像翻轉學習，從而加倍學習數據，但可能會影響帶有紋身、標記、疤痕等的角色。\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\" # @param [\".txt\",\".caption\"]\n",
        "#@markdown 混合動畫標籤有助於改進學習和提示輸入。啟用標籤時，激活標籤將被添加到每個文本檔案的開頭，並且不會被混合。\n",
        "shuffle_tags = True #@param {type:\"boolean\"}\n",
        "shuffle_caption = shuffle_tags\n",
        "activation_tags = \"3\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ▶️ 步驟 <p>\n",
        "#@markdown 您的圖片在訓練過程中將重複此次數。我建議圖片的數量乘以重複次數在 200 到 400 之間。\n",
        "num_repeats = 50 #@param {type:\"number\"}\n",
        "#@markdown 選擇您想要訓練的時長。一個不錯的起點是大約 10 個 epoch 或大約 2000 個步驟。<p>\n",
        "#@markdown 一個 epoch 的步驟數等於：圖片數量乘以重複次數，再除以批量大小。<p>\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown 保存更多的 epoch 可以更好地比較您的 LoRA 進度。\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ▶️ 學習\n",
        "#@markdown 學習率對結果非常重要。如果您有大量圖片，或者 dim 和 alpha 設定較高，請將 unet 的學習率設定為 2e-4 或更低。<p>\n",
        "#@markdown 文本編碼器有助於讓 LoRA 更好地學習概念。建議將其設為 unet 的一半或五分之一。如果您正在訓練風格，也可以設為 0。\n",
        "unet_lr = 1e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 1e-5 #@param {type:\"number\"}\n",
        "#@markdown 調度器是指導學習率的算法。如果不確定，選擇 \"constant\"，忽略數值。個人推薦使用 `cosine_with_restarts` 並設置 3 次重啟。\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "#@markdown 訓練期間，用於「預熱」學習率的步驟數以提高效率。建議設為 5%。\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
        "lr_warmup_steps = 0\n",
        "#@markdown 調整損失隨時間的變化，使學習更加高效。文檔建議設為 5.0，我建議動畫風格設為 8.0。更高的值會降低嚴格性，設為 0 可禁用。\n",
        "min_snr_gamma = 8.0 #@param {type:\"slider\", min:4.0, max:16.0, step:0.5}\n",
        "#@markdown MultiNoise 可以幫助改善顏色平衡（更深的黑色、更亮的白色）。但可能會影響眼睛的生成，僅在必要時使用。\n",
        "multinoise = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ▶️ 結構\n",
        "#@markdown LoRA 是經典類型，適用於多種用途。LoCon 更適合藝術風格，因為它有更多的層來學習數據集的更多細節。\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown 以下是針對 XL 設定的一些推薦值：\n",
        "\n",
        "#@markdown | 類型 | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | 角色 LoRA | 4 | 16 |   |   |\n",
        "#@markdown | 通用與風格 LoRA | 8 | 4 |   |   |\n",
        "#@markdown | 風格 LoCon | 16 | 8 | 16 | 8 |\n",
        "\n",
        "#@markdown 更高的 dim 意味著更大的 LoRA，可以容納更多信息，但更高並不總是更好。\n",
        "network_dim = 32 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "network_alpha = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "#@markdown 以下兩個值僅適用於 LoCon 的附加層。\n",
        "conv_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "conv_alpha = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### ▶️ 訓練\n",
        "#@markdown 根據您的 Colab 配置調整以下參數。\n",
        "#@markdown 如果您使用的是免費等級，請在本單元格頂部選擇一個 diffusers 模型。\n",
        "#@markdown\n",
        "#@markdown 批量大小（batch size）越大，訓練速度通常越快，但需要更多的記憶體。\n",
        "train_batch_size = 16 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown 我尚未發現 sdpa 和 xformers 之間有實質差異。\n",
        "cross_attention = \"sdpa\" #@param [\"sdpa\", \"xformers\"]\n",
        "#@markdown 如果您使用的是 A100 GPU，請啟用 bf16（僅適用於 Colab Pro）。\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\"]\n",
        "#@markdown 在 Google Drive 中緩存潛變數（Latent Caching）會在每張圖片旁生成一個 250 KB 的檔案，但能顯著降低記憶體使用量。\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "cache_latents_to_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown 以下選項會禁用 shuffle_tags 並停用文本編碼器的訓練。\n",
        "cache_text_encoder_outputs = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ▶️ 高級設置\n",
        "#@markdown 優化器是用於訓練的算法。默認的 AdaFactor 表現良好，而 Prodigy 自動管理學習率，具備多種優勢，例如需要較少的步驟即可完成訓練，對小型數據集效果更佳。\n",
        "optimizer = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "#@markdown AdamW8bit 的推薦參數：`weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
        "#@markdown Prodigy 的推薦參數：`decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`<p>\n",
        "#@markdown AdaFactor 的推薦參數：`scale_parameter=False relative_step=False warmup_init=False`\n",
        "#markdown CAME 的推薦參數：`weight_decay=0.04`\n",
        "optimizer_args = \"decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True\" #@param {type:\"string\"}\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "#@markdown 如果選擇 Dadapt 或 Prodigy 並勾選推薦設置，以下推薦值將覆蓋所有之前的設置：<p>\n",
        "#markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`<p>\n",
        "#markdown 如果選擇 CAME + REX 並勾選推薦設置，以下推薦值將覆蓋所有之前的設置：<p>\n",
        "#markdown `unet_lr=1e-4`, `text_encoder_lr=1e-6, betas=default_came_betas`\n",
        "recommended_values = True #@param {type:\"boolean\"}\n",
        "\n",
        "if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
        "  if recommended_values:\n",
        "    unet_lr = 0.75\n",
        "    text_encoder_lr = 0.75\n",
        "    network_alpha = network_dim\n",
        "elif \"CAME\" in optimizer:\n",
        "  optimizer = \"CAME\"\n",
        "  lr_scheduler = \"REX\"\n",
        "  if recommended_values:\n",
        "    unet_lr = 1e-4\n",
        "    text_encoder_lr = 1e-6\n",
        "    for i in range(len(optimizer_args)):\n",
        "      if \"betas\" in optimizer_args[i]:\n",
        "        optimizer_args.pop(i)\n",
        "        break\n",
        "\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "\n",
        "\n",
        "#@markdown ### ▶️ 準備\n",
        "#@markdown 現在您可以運行此單元格來訓練您的 Lora。祝你好運！ <p>\n",
        "\n",
        "\n",
        "# 👩‍💻 Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else pathlib.Path.home() / \"Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install torch==2.5.1+cu121 accelerate==0.19.0 transformers==4.30.2 diffusers==0.18.2 -q \\\n",
        "    bitsandbytes==0.40.0.post4 flax==0.7.5 opencv-python jax==0.4.23 jaxlib==0.4.23 -q \\\n",
        "    pytorch-lightning==1.9.0 voluptuous==0.13.1 toml==0.10.2 ftfy==6.1.1 einops==0.6.0 -q \\\n",
        "    safetensors pygments huggingface-hub==0.22.0 wandb invisible-watermark==0.2.0 open-clip-torch==2.20.0 -q \\\n",
        "    dadaptation==3.1 prodigyopt==1.0 lion-pytorch==0.1.2 -q\n",
        "  !pip install -e .\n",
        "  if cross_attention == \"xformers\":\n",
        "    !pip install -q xformers\n",
        "  if \"CAME\" in optimizer:\n",
        "    !pip install came-pytorch\n",
        "    !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_util.py -q -O library/train_util.py\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"LD_PRELOAD\"] = \"/content/libtcmalloc_minimal.so.4\"\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  if model_url.startswith(\"/content/drive/\") and not os.path.exists(model_url):\n",
        "    print(\"💥 Error: The custom training model you specified was not found in your Google Drive.\")\n",
        "    return\n",
        "\n",
        "  print(\"\\n💿 Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"💥 Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  # Find the folders and files\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"💥 Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  # Validation\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"💥 Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"💥 Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  test_files = []\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"💥 Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "    for ff in test_files:\n",
        "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
        "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
        "        print(f\"💥 Error: The files {f} and {ff} cannot have the same name. Aborting.\")\n",
        "        return\n",
        "    test_files.append(f)\n",
        "\n",
        "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
        "    caption_extension = \"\"\n",
        "\n",
        "  # Pretty stuff\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"📈 Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
        "  print(f\"📉 Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"🔮 There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "  else:\n",
        "    print(f\"🔮 There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"💥 Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n⭕ Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": cross_attention == \"xformers\",\n",
        "        \"sdpa\": cross_attention == \"sdpa\",\n",
        "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma > 0 else None,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"cache_latents\": cache_latents,\n",
        "        \"cache_latents_to_disk\": cache_latents_to_drive,\n",
        "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "        \"multires_noise_iterations\": 6 if multinoise else None,\n",
        "        \"multires_noise_discount\": 0.3 if multinoise else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"wandb_api_key\": wandb_key if wandb_key else None,\n",
        "        \"log_with\": \"wandb\" if wandb_key else None,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\n📄 Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"⭕ Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"📄 Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
        "  real_model_url = model_url  # There was a reason for having a separate variable but I forgot what it was.\n",
        "\n",
        "  if real_model_url.startswith(\"/content/drive/\"):\n",
        "    # Local model, already checked to exist\n",
        "    model_file = real_model_url\n",
        "    print(f\"📁 Using local model file: {model_file}\")\n",
        "    # Validation\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "      from safetensors.torch import load_file as load_safetensors\n",
        "      try:\n",
        "        test = load_safetensors(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "    elif model_file.lower().endswith(\".ckpt\"):\n",
        "      from torch import load as load_ckpt\n",
        "      try:\n",
        "        test = load_ckpt(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  else:\n",
        "    # Downloadable model\n",
        "    if load_diffusers:\n",
        "      if 'huggingface.co' in real_model_url:\n",
        "          match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', real_model_url)\n",
        "          if match:\n",
        "              username = match.group(1)\n",
        "              model_name = match.group(2)\n",
        "              model_file = f\"{username}/{model_name}\"\n",
        "              from huggingface_hub import HfFileSystem\n",
        "              fs = HfFileSystem()\n",
        "              existing_folders = set(fs.ls(model_file, detail=False))\n",
        "              necessary_folders = [ \"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\" ]\n",
        "              if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
        "                print(\"🍃 Diffusers model identified.\")  # Will be handled by kohya\n",
        "                return True\n",
        "      raise ValueError(\"💥 Failed to load Diffusers model. If this model is not Diffusers, have you tried turning it off at the top of the colab?\")\n",
        "\n",
        "    # Define local filename\n",
        "    if not model_file:\n",
        "      if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "        model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "      else:\n",
        "        model_file = \"/content/downloaded_model.safetensors\"\n",
        "        if os.path.exists(model_file):\n",
        "          !rm \"{model_file}\"\n",
        "\n",
        "    # HuggingFace\n",
        "    if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n",
        "      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "    # Civitai\n",
        "    elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url):\n",
        "      if m.group(2):\n",
        "        model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "      if m := re.search(r\"modelVersionId=([0-9]+)\", real_model_url):\n",
        "        real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "      else:\n",
        "        raise ValueError(\"💥 optional_custom_training_model contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "    # Download checkpoint\n",
        "    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "    # Download VAE\n",
        "    if not os.path.exists(vae_file):\n",
        "      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "      from safetensors.torch import load_file as load_safetensors\n",
        "      try:\n",
        "        test = load_safetensors(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "        !mv \"{model_file}\" \"{new_model_file}\"\n",
        "        model_file = new_model_file\n",
        "        print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "    if model_file.lower().endswith(\".ckpt\"):\n",
        "      from torch import load as load_ckpt\n",
        "      try:\n",
        "        test = load_ckpt(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"📂 Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "      return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "    t0 = time.time()\n",
        "    install_dependencies()\n",
        "    t1 = time.time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n✅ Installation finished in {int(t1-t0)} seconds.\")\n",
        "  else:\n",
        "    print(\"\\n✅ Dependencies already installed.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\n🔄 Getting model...\")\n",
        "    if not download_model():\n",
        "      print(\"\\n💥 Error: The model you specified is invalid or corrupted.\"\n",
        "            \"\\nIf you're using an URL, please check that the model is accessible without being logged in.\"\n",
        "            \"\\nYou can try civitai or huggingface URLs, or a path in your Google Drive starting with /content/drive/MyDrive\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\n🔄 Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n⭐ Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__.get('user_ns', {}).get('_exit_code', False):\n",
        "    display(Markdown(\"### ✅ Done! [Go download your Lora from Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                       \"### There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n",
        "\n",
        "main()\n",
        "\n",
        "#auto off\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido, el entorno se desconectara en 30 segundos\\033[0m\")\n",
        "    from google.colab import runtime\n",
        "    time.sleep(30)\n",
        "    runtime.unassign()\n",
        "else:\n",
        "    print(\"\\033[96mAuto off está desactivado, el entorno no se desconectará\\033[0m\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iRvqrBT3HtV"
      },
      "source": [
        "## *️⃣ 額外功能  \n",
        "您可以在開始訓練之前執行此操作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "23_YDYGhEnK0"
      },
      "outputs": [],
      "source": [
        "#@markdown 連接到 Google Drive 📁。以下的一些選項需要連接到 Google Drive，\n",
        "#@markdown 請在此處連接。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"連接到 Drive 成功！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODpo0kcX3KRy"
      },
      "source": [
        "### 📚 多個資料夾在同一數據集中  \n",
        "以下是一個範本，允許您在數據集中定義多個資料夾。您需要包括每個資料夾的位置，並可以為每個資料夾設置不同的重複次數。要添加更多資料夾，只需複製並粘貼以 `[[datasets.subsets]]` 開頭的部分。\n",
        "\n",
        "啟用此功能後，主單元格中設定的重複次數以及通過專案名稱設定的主資料夾將被忽略。\n",
        "\n",
        "您可以將其中一個設為正規化資料夾，只需添加 `is_reg = true`  \n",
        "還可以為每個資料夾單獨設置不同的 `keep_tokens`、`flip_aug` 等參數。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/KashiwazakiKana_IXL/dataset/21 25\"\n",
        "num_repeats = 25\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/KashiwazakiKana_IXL/dataset/CAUSAL 12 45\"\n",
        "num_repeats = 45\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/KashiwazakiKana_IXL/dataset/WORK 17 31\"\n",
        "num_repeats = 31\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ACrHePi3Pen"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🔢 計算數據集\n",
        "#@markdown 由於 Google Drive 無法直接計算資料夾中的文件數量，因此此功能將顯示所有資料夾及其子資料夾中的文件總數。\n",
        "\n",
        "folder = \"/content/drive/MyDrive/Loras/ejemplo/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"📂 Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"📁{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImAtduziVp5h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 重複計算器 ⌛📝\n",
        "#@markdown 計算用於訓練 Lora 的重複次數。請記住，在 `SDXL 和 Pony` 中，批量大小（batch size）為 `4`。\n",
        "#@markdown 如果您使用的是 Colab Pro，請以 `8` 的批量大小計算您的重複次數。\n",
        "\n",
        "# 定義變數\n",
        "# 圖片數量\n",
        "num_images = 40 # @param{type:\"number\"}\n",
        "# 重複次數\n",
        "num_repeats = 4 # @param{type:\"number\"}\n",
        "# epoch 數量\n",
        "num_epochs = 10 # @param{type:\"number\"}\n",
        "# 批量大小\n",
        "batch_size = 4 # @param{type:\"number\"}\n",
        "\n",
        "# 計算結果\n",
        "resultado = (num_images * num_repeats * num_epochs) / batch_size\n",
        "\n",
        "# 顯示結果\n",
        "print(\"\\33[96m總重複次數為:\\033[0m\", resultado)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YlP0cNQyujy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## HuggingFace 🤗\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown 創建一個 HuggingFace 帳戶 [點擊這裡](https://huggingface.co/)，如果您已有帳戶，直接使用您的 token 👇\n",
        "# @markdown > 獲取您的 HuggingFace **寫入權限** token [點擊這裡](https://huggingface.co/settings/tokens) 👈\n",
        "write_token = \"\"  # @param {type:\"string\"}\n",
        "#@markdown 如果您希望將文件上傳到您的組織，請填寫此處，否則保持空白即可。\n",
        "orgs_name = \"\"  # @param{type:\"string\"}\n",
        "# @markdown 如果您的模型/數據集存儲庫不存在，將自動創建。\n",
        "#@markdown 請勿使用空格，對於長名稱請使用 `底線`。\n",
        "\n",
        "#@markdown `model_name` 用於上傳單個文件。\n",
        "model_name = \"\"  # @param{type:\"string\"}\n",
        "#@markdown `dataset_name` 用於上傳整個文件夾。\n",
        "dataset_name = \"loras\"  # @param{type:\"string\"}\n",
        "#@markdown 勾選此框以將您的存儲庫/數據集設置為私有，否則它將是公開的。\n",
        "make_private = True  # @param{type:\"boolean\"}\n",
        "\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "\n",
        "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
        "    global model_repo\n",
        "    global datasets_repo\n",
        "\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
        "\n",
        "    if repo_type == \"model\":\n",
        "        model_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "    else:\n",
        "        datasets_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "if model_name:\n",
        "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
        "if dataset_name:\n",
        "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVrt9JJRCxmk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## 通過 huggingface_hub 上傳 (Lora) 🤗\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "file_path = \"/content/drive/MyDrive/Loras/Mi_proyecto/output/Mi_lora.safetensors\" #@param {type :\"string\"}\n",
        "#@markdown 評論（可選），可用於標識 Lora 的版本，例如：`pony, animagine, V1, V2 等...`\n",
        "\n",
        "commit_message = \"\"  #@param {type :\"string\"}\n",
        "\n",
        "if file_path != \"\":\n",
        "  path_obj = Path(file_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "  print(f\"Please wait...\")\n",
        "\n",
        "  api.upload_file(\n",
        "      path_or_fileobj=file_path,\n",
        "      path_in_repo=trained_model,\n",
        "      repo_id=model_repo,\n",
        "      commit_message=commit_message,\n",
        "  )\n",
        "\n",
        "  print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "print('¡¡Subida finalizada!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68XMHMLe--If",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ## 上傳資料夾 📁 至 HuggingFace 🤗\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# @markdown 您可以將整個資料夾上傳到 HuggingFace，包括訓練好的 Loras，甚至可以上傳整個專案資料夾（包括子資料夾）。請耐心等待，上傳多個文件可能需要一些時間。\n",
        "folder_path = \"/content/drive/MyDrive/Loras/Mi_lora/output\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown 上傳資料夾的自訂名稱\n",
        "folder_name = \"Mi_lora\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown 評論（可選）\n",
        "commit_message = \"\"  # @param {type :\"string\"}\n",
        "\n",
        "\n",
        "if not commit_message:\n",
        "    commit_message = \"feat: upload folder\"\n",
        "\n",
        "def upload_folder(folder_path, folder_name):\n",
        "    print(f\"Uploading {folder_name} to https://huggingface.co/datasets/{datasets_repo}\")\n",
        "    print(\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=folder_path,\n",
        "        path_in_repo=folder_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Carga terminada, puedes encontrarlo en https://huggingface.co/datasets/{datasets_repo}/tree/main/{folder_name}\\n\")\n",
        "\n",
        "def upload():\n",
        "    if folder_path:\n",
        "        upload_folder(folder_path, folder_name)\n",
        "\n",
        "upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RfwB4UB3M_w",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d819c2-0686-4ad6-9e89-708ec18e0232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Connecting to Google Drive...\n",
            "Mounted at /content/drive\n",
            "✅ Done\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### 📂 解壓數據集\n",
        "#@markdown 在 Google Drive 上傳單個文件的速度較慢，因此如果您在電腦上有數據集，可以將其壓縮為 ZIP 文件並上傳。在這裡，您可以解壓縮它。\n",
        "zip = \"/content/drive/MyDrive/ABC.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/ABC/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"📂 Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"✅ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7CL4Ckp3SeI"
      },
      "source": [
        "# 📈 繪製訓練結果  \n",
        "在運行訓練器後可以執行此操作。如果您不熟悉此功能，則無需使用。  \n",
        "第一個單元可能無法加載所有記錄。請繼續嘗試第二個單元，直到所有數據都被加載。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TRI3eX90Rp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rM5SLq990Rp"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}